"""
—Ä–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ —Ç–µ—Ä–º–∏–Ω–∞–ª –ø–æ—Å–ª–µ conda activate llama_env
(llama_env) (tun_llm) bolkhovskiydmitriy@MacBook-Pro-Bolkhovskiy-2 LLM_RAG_BOT % python3


"""
from pyexpat.errors import messages

# -*- coding: utf-8 -*-
# 100% —Ä–∞–±–æ—á–∞—è –≤–µ—Ä—Å–∏—è –ø–æ–¥ MacBook Pro M1/M2 16 –ì–ë (–Ω–æ—è–±—Ä—å 2025)

from bootstrap import *

correct_url_goods = (U24.data2Df_upload(root_path + '/PROJECTS/LLM_OIL_CLASSIFIER/data_in/alleya_lavr_to_json.xlsx')[
                              'url'].unique())

import os
import json, re
import faiss
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama
from datetime import datetime as DT
from aiogram.utils.markdown import hlink

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3", use_fast=True)



LORA_PATH  = '/PROJECTS/LLM_OIL_CLASSIFIER/data_in/adapter_liquimoly.gguf'

path_rag_index = root_path + '/PROJECTS/LLM_OIL_CLASSIFIER/data_in/oil_rag_index/'
INDEX_FP = os.path.join(path_rag_index, "faiss.index")
META_FP  = os.path.join(path_rag_index, "meta.jsonl")

# ==============================================================================
# –ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–µ—Ä–∞ (—Å–∞–º—ã–π –ª—ë–≥–∫–∏–π –∏ –±—ã—Å—Ç—Ä—ã–π –Ω–∞ M1)
# ==============================================================================
print("–ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–µ—Ä–∞ multilingual-e5-base...")
import torch
import platform

#–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —É—Å—Ç—Ä–æ–π—Ç—Å–≤–∞ –¥–ª —è—Ä–∞–∑–Ω—ã—Ö –û–° (Mac/Win) gpu/cpu
if platform.system() != "Darwin":  # –µ—Å–ª–∏ –Ω–µ macOS
    mps_availability = False
else:
    mps_availability = torch.backends.mps.is_available()

if torch.cuda.is_available():
    device = "cuda"
elif mps_availability:
    device = "mps"
else:
    device = "cpu"

print(f"[E5] –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device} (OS: {platform.system()})")

E5 = SentenceTransformer(
    "intfloat/multilingual-e5-base",
    device=device,
    cache_folder="./models_cache"
)
# ==============================================================================
# –ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞
# ==============================================================================
print("–ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
index = faiss.read_index(INDEX_FP)
with open(META_FP, "r", encoding="utf-8") as f:
    meta = [json.loads(l) for l in f if l.strip()]

# ==============================================================================
# –ó–∞–≥—Ä—É–∑–∫–∞ Llama 3.1 8B Q5_K_M + LoRA ‚Äî –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û –ø–æ–¥ 16 –ì–ë
# ==============================================================================
print("–ó–∞–≥—Ä—É–∑–∫–∞ Llama 3.1 8B Q5_K_M + LoRA (—ç—Ç–æ –∑–∞–π–º—ë—Ç 15‚Äì25 —Å–µ–∫)...")




# ==============================================================================
# RAG —Ñ—É–Ω–∫—Ü–∏–∏
# ==============================================================================
def retrieve(query: str, k: int = 6):
    q_vec = E5.encode(
        [f"query: {query}"],
        normalize_embeddings=True,
        batch_size=1,
        show_progress_bar=False
    ).astype("float32")

    D, I = index.search(q_vec, k + 2)  # –±–µ—Ä—É —á—É—Ç—å –±–æ–ª—å—à–µ –Ω–∞ —Å–ª—É—á–∞–π –º—É—Å–æ—Ä–∞
    hits = []
    for i, idx in enumerate(I[0]):
        if idx == -1:
            continue
        doc = meta[int(idx)]
        hits.append({"rank": i+1, "score": float(D[0][i]), **doc})
    return hits[:k]


# –§—É–Ω–∫—Ü–∏—è: –û—á–∏—Å—Ç–∫–∞ –¥–ª—è Telegram
def clean_text_for_telegram(text: str) -> str:
    if not text:
        return ""
    # –£–¥–∞–ª—è–µ–º HTML-—Ç–µ–≥–∏
    text = re.sub(r'<[^>]+>', '', text)

    # üí• –ò–ó–ú–ï–ù–ï–ù–ò–ï –ó–î–ï–°–¨:
    # –ò—â–µ–º –≤—Å–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å–æ—Å—Ç–æ—è—â–∏–µ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö
    # –ø—Ä–æ–±–µ–ª–æ–≤, –Ω–æ –ò–°–ö–õ–Æ–ß–ê–Ø –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ (\n).
    text = re.sub(r'[ \t]+', ' ', text).strip()  # –ó–∞–º–µ–Ω—è–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã (–ø—Ä–æ–±–µ–ª, —Ç–∞–±—É–ª—è—Ü–∏—è)

    # –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –Ω–∞ –æ–¥–∏–Ω –∏–ª–∏ –¥–≤–∞, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –≥–∏–≥–∞–Ω—Ç—Å–∫–∏—Ö –ø—É—Å—Ç–æ—Ç
    text = re.sub(r'(\n\s*){3,}', '\n\n', text)

    # –¢–æ–ª—å–∫–æ —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã (—Å—Ç—Ä–æ–∫–∞ 75 –≤ –≤–∞—à–µ–º –∫–æ–¥–µ –±—ã–ª–∞ —Ç—É—Ç)
    # text = re.sub(r'\s+', ' ', text).strip() # <-- –≠–¢–£ –°–¢–†–û–ö–£ –ù–£–ñ–ù–û –£–î–ê–õ–ò–¢–¨ –ò–õ–ò –ó–ê–ö–û–ú–ú–ï–ù–¢–ò–†–û–í–ê–¢–¨!

    # –û–±—Ä–µ–∑–∞–µ–º –ø–æ –ª–∏–º–∏—Ç—É
    if len(text) > 4090:
        text = text[:4085] + "..."

    return text.strip()


def generate_Answer(question: str, llm, k: int = 6):
    if (len(question) < 12) or (len(question.split()) < 3):
        return {"answer": "–ß—Ç–æ–±—ã –ò–ò —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ –≤–æ–ø—Ä–æ—Å –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ", "sources": "not_correct_question"}

    statement_lst = ['–ü–æ—Å—Ç–∞—Ä–∞–π—Å—è –æ—Ç–≤–µ—á–∞—Ç—å –ø—Ä–æ —Ä—É—Å—Å–∫–∏, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∏ –¥—Ä—É–≥–∏–µ —è–∑—ã–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ—Ä–º–∏–Ω–æ–≤, –±—Ä–µ–Ω–¥–æ–≤ –∞—Ä—Ç–∏–∫—É–ª–æ–≤ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π']
    for statement in statement_lst:
        question += statement
    contexts = retrieve(question, k=k)

    context_lines = [c.get("answer", "").strip() for c in contexts if c.get("answer")]
    context_str = "\n".join(f"- {line}" for line in context_lines)

    if not context_str.strip():
        context_str = "–ö–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç."

    # === –§–ò–ö–°: –û–ë–†–ï–ó–ê–ï–ú –ö–û–ù–¢–ï–ö–°–¢ ===
    MAX_CONTEXT_CHARS = 12000  # ~3000 —Ç–æ–∫–µ–Ω–æ–≤, –±–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è n_ctx=4096
    if len(context_str) > MAX_CONTEXT_CHARS:
        lines = context_str.split('\n')
        truncated_lines = []
        current_len = 0
        for line in reversed(lines):  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω–µ—Ü (–ª—É—á—à–∏–µ –º–∞—Ç—á–∏)
            line_len = len(line) + 1  # +1 –¥–ª—è \n
            if current_len + line_len < MAX_CONTEXT_CHARS:
                truncated_lines.append(line)
                current_len += line_len
            else:
                break
        context_str = '\n'.join(reversed(truncated_lines)) + "\n... (–∫–æ–Ω—Ç–µ–∫—Å—Ç —É–∫–æ—Ä–æ—á–µ–Ω)"
        print(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–æ {len(context_str)} —Å–∏–º–≤–æ–ª–æ–≤")  # –î–ª—è –¥–µ–±–∞–≥–∞

    system_prompt = (
        "–¢—ã ‚Äî –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –≠–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–≤—Ç–æ—Ö–∏–º–∏–∏ –∏ –∞–≤—Ç–æ–º–∞—Å–ª–∞–º. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –¥–∞—Ç—å —á–µ—Ç–∫–∏–π –∏ –ª–∞–∫–æ–Ω–∏—á–Ω—ã–π –æ—Ç–≤–µ—Ç. "
        "–û—Ç–≤–µ—á–∞–π –°–¢–†–û–ì–û –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –¢–û–õ–¨–ö–û —Ñ–∞–∫—Ç—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞." #, –Ω–µ –≤—ã–¥—É–º—ã–≤–∞–π –∏ –Ω–µ –≥–∞–ª—é—Ü–∏–Ω–∏—Ä—É–π
        "–û—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ 3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã–º–∏."
        "–ü–æ—Å—Ç–∞—Ä–∞–π—Å—è –¥–∞—Ç—å —Å—Å—ã–ª–∫—É –Ω–∞ —Ç–æ–≤–∞—Ä—ã, –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –ø—Ä–∏ —ç—Ç–æ–º, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤ –Ω–∞—á–∞–ª–µ –æ—Ç–≤–µ—Ç–∞ –¥–∞–≤–∞–π –ø–æ—è—Å–Ω–µ–Ω–∏—è: –æ–ø–∏—Å–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–∏–º–Ω–µ–Ω–µ–Ω–∏—é."
        "–ö–ê–¢–ï–ì–û–†–ò–ß–ï–°–ö–ò –ó–ê–ü–†–ï–©–ï–ù–û: "
        "1. –û–±—Å—É–∂–¥–∞—Ç—å –∏–ª–∏ –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–≤–æ–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –ø—Ä–æ—Ü–µ—Å—Å –≤—ã–±–æ—Ä–∞, –∏–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç. "
        "2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–≤–æ–¥–Ω—ã–µ —Ñ—Ä–∞–∑—ã, –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è, –æ—Ü–µ–Ω–æ—á–Ω—ã–µ —Å—É–∂–¥–µ–Ω–∏—è, –∏–ª–∏ –æ–ø—Ä–∞–≤–¥–∞–Ω–∏—è. "
        "3. –ù–∞—á–∏–Ω–∞—Ç—å –æ—Ç–≤–µ—Ç —Å–ª–æ–≤–∞–º–∏ —Ç–∏–ø–∞ '–í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ —è –±—É–¥—É –æ—Ç–≤–µ—á–∞—Ç—å...' –∏–ª–∏ '–í –æ—Ç–≤–µ—Ç–µ –±—É–¥—É—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ...'. "
        )

    try:


        full_prefix = system_prompt + "\n–í–æ–ø—Ä–æ—Å: " + question + "\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n"
        prefix_tokens = len(tokenizer.encode(full_prefix))

        if prefix_tokens + 100 > 3800:  # –µ—Å–ª–∏ –¥–∞–∂–µ –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —É–∂–µ –º–Ω–æ–≥–æ
            context_str = "–ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –æ–±—ä—ë–º–Ω—ã–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏."
        elif len(tokenizer.encode(full_prefix + context_str)) > 3800:
            max_context_tokens = 3800 - prefix_tokens - 50  # –∑–∞–ø–∞—Å
            context_tokens = tokenizer.encode(context_str)[:max_context_tokens]
            context_str = tokenizer.decode(context_tokens, skip_special_tokens=True)
            print(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –ø–æ —Ç–æ–∫–µ–Ω–∞–º –¥–æ {max_context_tokens}")

    except ImportError:
        # –ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –±–µ–∑ transformers
        if len(context_str) > 12000:
            context_str = context_str[-12000:] + "\n... (–∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ–∫—Ä–∞—â—ë–Ω)"



    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"–í–æ–ø—Ä–æ—Å: {question}\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context_str}"}
    ]




    output = llm.create_chat_completion(
        messages=messages,
        max_tokens=350,
        temperature=0.2,
        top_p=0.90,
        top_k=40,
        repeat_penalty=1.12,
        # üí• –ù–æ–≤—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–æ–±–∞–≤–ª–µ–Ω—ã –∑–¥–µ—Å—å
        stop=["<|eot_id|>", "<|end_of_text|>", "–í–æ–ø—Ä–æ—Å:", "–ö–æ–Ω—Ç–µ–∫—Å—Ç:", "–í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ —è –±—É–¥—É", "–í –æ—Ç–≤–µ—Ç–µ –±—É–¥—É—Ç"],
        stream=False,
    )

    answer = output["choices"][0]["message"]["content"].strip()


    def urlControl(answer, contexts):
        # === –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ===
        url_lst = []
        for c in contexts:
            possible_url = c.get("url") or c.get("goods_href") or c.get("link") or c.get("href")
            url_lst.append(possible_url.strip())

        unique_urls = list(dict.fromkeys(url_lst))  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫, —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
        unique_urls = [url for url in unique_urls if url in correct_url_goods]

        # # === –ì–õ–ê–í–ù–û–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –¥–∞–ª–∞ –ª–∏ –º–æ–¥–µ–ª—å —Å—Å—ã–ª–∫–∏ —Å–∞–º–∞ ===
        #     # –ú–æ–¥–µ–ª—å –ù–ï –¥–∞–ª–∞ —Å—Å—ã–ª–æ–∫ ‚Üí –¥–æ–±–∞–≤–ª—è–µ–º –º—ã
        if len(unique_urls) == 1:
            answer += f"\n\n–°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä: {unique_urls[0]}"
        else:
            answer += "\n\n–°—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã:\n" + "\n".join(unique_urls[:3])

        answer = ' '.join(x for x in answer.split() if (not 'https://' in x) or (x in correct_url_goods))

        answer = answer.replace("–°—Å—ã–ª–∫–∞ –Ω–∞", '|').replace("–°—Å—ã–ª–∫–∏ –Ω–∞", '|').split("|")
        answer = [x for x in answer if (len(x) > 25) or ('https://' in x)]
        answer_txt = [x for x in answer if 'https://' not in x]
        answer_url = [x for x in answer if 'https://' in x]

        answer_txt = '\n'.join(answer_txt)
        lst_url = []
        for x in answer_url:
            x = x.split()
            for y in x:
                if 'https://' in y:
                    lst_url.append(y)

        answer_url = "\n\n–¢–æ–≤–∞—Ä—ã:\n" + "\n\n".join(lst_url)
        answer = answer_txt + answer_url

        return answer

    answer = urlControl(answer, contexts)


    # === –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è Telegram ===
    answer = clean_text_for_telegram(answer)

    return {"answer": answer, "sources": contexts[:k]}


# ==============================================================================
# –¢–ï–°–¢ (–º–æ–∂–Ω–æ –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Ç–æ–º)
# ==============================================================================
if __name__ == "__main__":
    test_questions = [
    "volvo s60 2010 –º–∞—Å–ª–æ —Ç—Ä–∞–Ω—Å–º–∏—Å–∏–æ–Ω–Ω–æ–µ",
        "–ö–∞–∫–æ–π –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å —Ä–∂–∞–≤—á–∏–Ω—ã –≤—ã–±—Ä–∞—Ç—å –¥–∞–π –æ–ø–∏—Å–∞–Ω–∏–µ –∏ —Å—Å—ã–ª–∫—É –Ω–∞ —Ç–æ–≤–∞—Ä?",
    "–ö–∞–∫ —É–¥–∞–ª–∏—Ç—å —Ä–µ–∫–ª–∞–º–Ω—É—é –Ω–∞–∫–ª–µ–π–∫—É —Å –∫—É–∑–æ–≤–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—è?",
    "–ß–µ–º –ø–æ—á–∏—Å—Ç–∏—Ç—å –∫–∞—Ä–±—é—Ä–∞—Ç–æ—Ä?",
    "–ó–∞–º—ë—Ä–∑ –∑–∞–º–æ–∫ –≤ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ, —á—Ç–æ –¥–µ–ª–∞—Ç—å?",
    "–û–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ Ln1733. –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏ —Å—Å—ã–ª–∫–∞ url —Ç–æ–≤–∞—Ä–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏",
    "–ß–µ–º –ø–æ—á–∏—Å—Ç–∏—Ç—å –ø—è—Ç–Ω–æ –æ—Ç —á–∞—è –Ω–∞ –¥–∏–≤–∞–Ω?",
    "–ù—É–∂–Ω—ã –ª–∏ –∞–Ω—Ç–∏–∫–æ—Ä—Ä–æ–∑–∏–π–Ω—ã–µ –ø—Ä–∏—Å–∞–¥–∫–∏ –¥–ª—è –∑–∞—â–∏—Ç—ã —Ç–æ–ø–ª–∏–≤–Ω–æ–≥–æ –±–∞–∫–∞?",
    "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç –ø—Ä–∏—Å–∞–¥–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –≥–∏–¥—Ä–æ–∫–æ–º–ø–µ–Ω—Å–∞—Ç–æ—Ä–æ–≤?",
    "–°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≤ –≥–æ–¥ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å –ø—Ä–∏—Å–∞–¥–∫–∏ –≤ —Ç—Ä–∞–Ω—Å–º–∏—Å—Å–∏–æ–Ω–Ω–æ–µ –º–∞—Å–ª–æ –ê–ö–ü–ü?",
    "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –ª–∏ –¥–µ–ø—Ä–µ—Å—Å–æ—Ä–Ω—ã–µ –ø—Ä–∏—Å–∞–¥–∫–∏ –ø—Ä–æ—Ç–∏–≤ –∑–∞–º–µ—Ä–∑–∞–Ω–∏—è –¥–∏–∑–µ–ª—å–Ω–æ–≥–æ —Ç–æ–ø–ª–∏–≤–∞ –∑–∏–º–æ–π?",
    "–ö–∞–∫ –ø—Ä–æ–º—ã—Ç—å —Ä–∞–¥–∏–∞—Ç–æ—Ä —Å–∏—Å—Ç–µ–º—ã –æ—Ö–ª–∞–∂–¥–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –ø—Ä–∏—Å–∞–¥–∫–∏?",
    "–ö–∞–∫–∏–µ –æ—á–∏—Å—Ç–∏—Ç–µ–ª–∏ –∫–∞—Ä–±—é—Ä–∞—Ç–æ—Ä–∞ —Å—Ç–æ–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –∏–Ω–∂–µ–∫—Ç–æ—Ä–Ω—ã—Ö –º–æ—Ç–æ—Ä–æ–≤?",
    "–ü—Ä–∏—Å–∞–¥–∫–∏ –¥–ª—è —Ü–µ–ø–∏ –ì–†–ú: –ø–æ–ª—å–∑–∞ –∏ –≤–æ–∑–º–æ–∂–Ω—ã–π –≤—Ä–µ–¥?",
    "–ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –ø—Ä–∏—Å–∞–¥–∫—É –¥–ª—è —Å–º–µ—à–∞–Ω–Ω–æ–≥–æ —Ç–æ–ø–ª–∏–≤–∞ (–±–µ–Ω–∑–∏–Ω + —ç—Ç–∞–Ω–æ–ª)?"
]


    ##################
    from llama_cpp import Llama

    model_dir = root_path + '/DATA_CATALOGS/llm_models/gguf/'

    # MODEL_PATH = model_dir + '//Qwen2.5-Coder-3B-Instruct.Q5_K_M.gguf' #–≥–æ—Ä–∞–∑–¥–æ —Ö—É–∂–µ Mistral
    # MODEL_PATH = model_dir + '/DeepSeek-R1-Distill-Qwen-7B.Q4_K_M.gguf' #–≥–æ—Ä–∞–∑–¥–æ —Ö—É–∂–µ Mistral
    ## –ü–æ–ª–Ω—ã–π –æ—Ç—Å—Ç–æ–π: Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf, Phi-3-mini-4k-instruct (Microsoft)

    MODEL_PATH = model_dir + '/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf'  # –Ω–∞–∏–ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏

    MAX_len_param_df = 25  # –≤ —è—á–µ–π–∫–µ A_text –≤ —Å—Ä–µ–¥–Ω–µ–º 400 —Å–∏–º–≤–æ–ª–æ–≤ - –∫–æ–ª-–≤–æ —Å—Ç—Ä–æ–∫ –±–æ–ª—å—à–µ, –∑–Ω–∞—á–∏—Ç —Å—Ä–µ–∑ –Ω–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π - –æ–±—â–∏–π –ø–æ–∏—Å–∫
    LIM_len_param_df = 13

    llm = Llama(
        model_path=MODEL_PATH,
        n_ctx=8192,  # 8192
        n_batch=1024,
        n_gpu_layers=99,
        n_threads=10,
        verbose=False,
    )

    ##################

    T1 = U24.tNow()
    for i, query in enumerate(test_questions):
        tt1 = U24.tNow()
        result = generate_Answer(query, llm, k=4)
        answer = result["answer"]
        print(f"\ntime_delta = {(U24.tNow() - tt1).total_seconds()}\n–∑–∞–ø—Ä–æ—Å: {query}\n–æ—Ç–≤–µ—Ç: {answer}\n{'=' * 60}")

    print(f"~ —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ 1 –∑–∞–ø—Ä–æ—Å: {((U24.tNow() - T1).total_seconds()) // (i + 1)}")

